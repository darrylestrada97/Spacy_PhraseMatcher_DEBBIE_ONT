{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0bc8c1-b8d6-4829-9b37-80df1eab7f36",
   "metadata": {},
   "source": [
    "In this Jupyter notebook, we will explore the use of Spacy's PhraseMatcher with RapidFuzzy search to identify terms from a list of terms in a given text. Spacy is a popular open-source software library used for advanced Natural Language Processing (NLP) tasks, while RapidFuzzy is a fast and efficient fuzzy search library in Python.\n",
    "\n",
    "The ability to quickly and accurately identify terms from a list in a text is a crucial task in NLP applications such as document classification, sentiment analysis, and named entity recognition. The combination of Spacy's PhraseMatcher and RapidFuzzy search can greatly simplify this task and reduce the computational time needed.\n",
    "\n",
    "Throughout this notebook, we will walk through the steps to create a PhraseMatcher object, use RapidFuzzy search to compare the extracted phrases with a list of terms, and finally visualize the results. By the end of this notebook, you will have a better understanding of how to use Spacy and RapidFuzzy together to efficiently identify terms in a given text.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f384d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create term list should look like this [{\"ID\": 1, \"term\": \"term1\", \"label\": \"label1\"}, {\"ID\": 2, \"term\": \"term2\", \"label\": \"label2\"}, ... {\"ID\": 30000, \"term\": \"term30000\", \"label\": \"label30000\"}\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from multiprocessing  import Pool\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "\n",
    "list_of_dictionaries = [dic for dic in os.listdir(\"DEBBIE_dictionaries_annotations-main/dictionaries\") if dic.startswith(\".\") is False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93e29243",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = []\n",
    "for dictionary in list_of_dictionaries:\n",
    "    try:\n",
    "        data = pd.read_csv(\"DEBBIE_dictionaries_annotations-main/dictionaries/\"+dictionary,sep=\"\\t\",usecols=[0,1,2], names=['term', 'label',\"id\"], header=None)\n",
    "        data = data.dropna()\n",
    "        data = data.to_dict(orient=\"records\")\n",
    "        for d in data:\n",
    "            d[\"label\"] = d[\"label\"].replace(\"LABEL=\", \"\") # remove the \"LABEL=\" string from the \"label\" key\n",
    "            d[\"id\"] = d[\"id\"].replace(\"ID=\", \"\") # remove the \"ID=\" string from the \"id\" key\n",
    "        terms += data \n",
    "    except:\n",
    "        print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d518918e-3868-493b-a7e7-2bd5939427e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'term': 'CCL1', 'label': 'Cell', 'id': 'id.200906009267275401'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms[54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c02931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patterns(term):\n",
    "    try:\n",
    "        pattern = nlp(term[\"term\"])\n",
    "        fuzzy_pattern = [{\"LOWER\": token.lower_, \"FUZZY\": term[\"term\"]} for token in pattern]\n",
    "        return (fuzzy_pattern + [{\"ID\": term[\"id\"], \"LABEL\": term[\"label\"]}, pattern])\n",
    "    except:\n",
    "        print(\"error with term: \"+str(term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81176902",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") # load the small English model\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab) # initialize the PhraseMatcher object\n",
    "#terms = [{\"ID\": 1, \"term\": \"term1\", \"label\": \"label1\"}, {\"ID\": 2, \"term\": \"term2\", \"label\": \"label2\"}, ... {\"ID\": 30000, \"term\": \"term30000\", \"label\": \"label30000\"}] # list of dictionaries containing terms to match (30000 terms)\n",
    "\n",
    "# create a list of patterns that includes the original term and its fuzzy match, as well as the term ID and label\n",
    "patterns = []\n",
    "# for term in terms:\n",
    "#     pattern = nlp(term[\"term\"])\n",
    "#     fuzzy_pattern = [{\"LOWER\": token.lower_, \"FUZZY\": term[\"term\"]} for token in pattern]\n",
    "#     patterns.append(fuzzy_pattern + [{\"ID\": term[\"id\"], \"LABEL\": term[\"label\"]}, pattern])\n",
    "with Pool(6) as pool:\n",
    "    patterns += pool.map(create_patterns,terms)\n",
    "matcher.add(\"TERMS\", None, *patterns)\n",
    "\n",
    "# example text to match against\n",
    "text = \"Here is an example text that contains some of the terms we want to match.\"\n",
    "\n",
    "doc = nlp(text) # create a Doc object from the example text\n",
    "matches = []\n",
    "for pattern in patterns:\n",
    "    for match in matcher(doc):\n",
    "        if match[0] == nlp.vocab.strings[\"TERMS\"]:\n",
    "            match_text = doc[match[1]:match[2]].text.lower()\n",
    "            if fuzz.partial_ratio(match_text, pattern[0][\"FUZZY\"]) > 80:\n",
    "                matches.append(match + pattern[-2:])\n",
    "\n",
    "# print the matches\n",
    "for match_id, start, end, metadata, _ in matches:\n",
    "    print(f\"Matched term: {doc[start:end].text}\")\n",
    "    print(f\"ID: {metadata['ID']}\")\n",
    "    print(f\"Label: {metadata['LABEL']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599558c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf37111",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns_2 = []\n",
    "\n",
    "for n in terms:\n",
    "    patter = {}\n",
    "    pattern = []\n",
    "    text = n[\"term\"].split(\" \")\n",
    "    for n_t in text:\n",
    "        pattern.append({\"LOWER\":n_t.lower()})\n",
    "    patter[\"id\"] = n[\"id\"]\n",
    "    patter[\"label\"] = n[\"label\"]\n",
    "    patter[\"pattern\"] = pattern\n",
    "    patterns_2.append(patter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f431699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = nlp.add_pipe(\"entity_ruler\",validate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16bb8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=\"\"\"1998 Jan\n",
    "Bioactive glass fiber/polymeric composites bond to bone tissue. \n",
    "Bioactive glass fibers were investigated for use as a fixation vehicle between a low modulus, polymeric composite and bone tissue. In an initial pilot study, bioactive glass fiber/polysulfone composites and all polysulfone control rods were implanted into the rabbit tibia; the study was subsequently expanded with implantation into the rabbit femur. Bone tissue exhibited direct contact with the glass fibers and adjacent polymer matrix and displayed a mechanical bond between the composite and bone tissue after six weeks implantation. Interfacial bond strengths after six weeks implantation averaged 12.4 MPa, significantly higher than those of the all polymer controls. Failure sites for the composite at six weeks generally occurred in the bone tissue or composite, whereas the failure site for the polymer implants occurred exclusively at the implant/tissue interface. The bioactive glass fiber/polysulfone composite achieved fixation to bone tissue through a triple mechanism: a bond to the bioactive glass fiber, mechanical interlocking between the tissue and glass fibers, and close apposition and possible chemical bond between the portions of the polymer and bone tissue. This last mechanism resulted from an overspill of bioactivity reactions from the fibers onto the surface of the surrounding polymer which we call the \"halo\" effect. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e29c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler.add_patterns(patterns_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdc6302",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2fc7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.remove_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cd9f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.label_,ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7728f382",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    if d[\"term\"]:\n",
    "        print(\"Empty value found for 'term' key in dictionary:\", d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c22c085-3189-4ba2-b221-732905792786",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [n for n in os.listdir(\"/jupyter/Miguel/prodigy_brat_files\") if n.startswith(\".\") == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daab5d88-4038-4686-a099-c12317645f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for direc in dirs:\n",
    "    path = \"/jupyter/Miguel/prodigy_brat_files/\"+direc\n",
    "    docs = [n for n in os.listdir(path) if n.endswith(\"txt\") == True]\n",
    "    for doc in docs:\n",
    "        name = doc.split(\".\")[0]\n",
    "        with open(path+\"/\"+name+\".txt\",\"r\") as doc1:\n",
    "            text = doc1.read()\n",
    "            doc2 = nlp(text)\n",
    "        annotations_list = []   # list to store annotations\n",
    "        T_id = 1   # brat annotation line id\n",
    "\n",
    "        # Go through the predicted entities\n",
    "        if doc2.ents:\n",
    "            with open(\"/jupyter/Miguel/DEBBIE/\"+name+\".txt\",\"a\") as doc3:\n",
    "                for ent in doc2.ents:\n",
    "                    doc3.write('T{}\\t{} {} {}\\t{}\\n'.format(T_id, ent.label_, ent.start_char, ent.end_char, ent.text))\n",
    "                    T_id += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
